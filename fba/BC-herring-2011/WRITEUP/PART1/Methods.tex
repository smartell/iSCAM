%!TEX root = /Users/stevenmartell/Documents/CURRENT PROJECTS/iSCAM-trunk/fba/BC-herring-2011/WRITEUP/BCHerring2011.tex
\section{Methods}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	

	\subsection{Analytical methods}
	We present a new stock assessment platform for the assessment of BC Pacific herring. This platform is based on the general statistical catch age model first described by \cite{fournier1982general}.  The software platform used is called \iscam, which stands for integrated Statistical Catch Age Model.  The source code and documentation for \iscam\ is freely available from \url{https://sites.google.com/site/iscamproject/}, or from a subversion repository at \url{http://code.google.com/p/iscam-project/}.  The subversion repository is more likely to be up to date; whereas, the project website has periodic updates with corresponding version numbers.  Ideally, the results of this report could easily be repeated just by downloading the necessary software and using the data and control files presented in the appendix of this paper. A complete technical description of \iscam\ is provided in Appendix \ref{appiSCAM} of this report.
	
	In short, for each stock  two input files are required for \iscam: (1) a data file that contains the historical catch, survey, life-history and age-composition information, and (2) a control file that specifies initial parameter values, priors, selectivity options, and various other controls that specify options for time-varying natural mortality, type of recruitment model, etc.  Each major and minor stock has its own data and control file and these are provided in Appendix \ref{AppendixDataFiles} such that these results can be verified by an independent reviewer using the \iscam\ software.
	
	Estimated model parameters includes the initial numbers-at-age, annual age-2 recruits, annual fishing mortality rates for each gear, selectivity parameters, natural mortality rates, parameters that describe the observation error and process error variance, and the unfished age-2 recruits and the steepness of the stock recruitment relationship.  The total number of estimated parameters differs for each stock assessment region depending on the number of years of active fishing, assumptions about selectivity, and the number of assumed nodes in natural mortality rates.  
	
	\subsection{Sustainable Fisheries Framework reference points}

The Sustainable Fisheries Framework provides the basis for ensuring Canadian fisheries are conducted in a manner which support conservation and sustainable use. The framework incorporates existing fisheries management policies along with new and evolving policies.  The framework consists of two main elements: conservation and sustainable use policies, and planning and monitoring tools.  The conservation and sustainable use policies incorporate the precautionary and ecosystem approaches to fisheries management. 


The general framework for developing a harvest strategy that is compliant with the precautionary approach is to divide the stock into three stock status zones; healthy, cautious, and critical zones \citep{dfo2006}.  In this work we define these stock status thresholds as 0.4\bmsy\ and 0.8\bmsy\ when crossing from the critical-cautious zone and cautious-healthy zone, respectively.  A critical component to this interpretation is the definition of \bmsy.  In the case of a single fishing fleet using a fixed gear, \bmsy\ is normally defined as the spawning stock biomass that would, on average, support the largest surplus yield.  In the case of multiple fishing fleets, each using a different gear with different selectivities, the definition of \bmsy\ is more complex and is a function of allocation of mortality to each gear type.  For example, if one gear harvest fish at a much younger age than the other gear, an increase in allocation to the gear that catches younger sexually immature fish will shift \bmsy\ upwards. Also, changes in selectivity over time (perhaps due to changes in growth) will result in changing \bmsy.  This precise definition for \bmsy\ increases the difficulty in estimating reference points when there are non-stationary parameters in the model (i.e., time-varying natural mortality rates, selectivity, growth in the case of Pacific herring).
	
	Estimates of reference points are based on equilibrium calculations (see Appendix \ref{appiSCAM}), where the average natural mortality rate over the time period in question is used along with the most recent empirical estimates of weight-at-age and fecundity.  In lieu of any formal allocation arrangements for the gear-types that harvest herring, the ratios of average catch over the past 20 years for each gear type, in each SAR, are used for allocation purposes and calculating MSY based reference points. 
	
	
	

	
	\subsection{Simulation testing}
	
		The purpose of conducting simulation testing is two fold: (1) to demonstrate that the model is capable of estimating model parameters given perfect information, and (2) to examine precision and bias in parameter estimates (and corresponding management quantities) in the presence of observation and process errors.  To conduct simulation testing using \iscam, the following command line option \verb"-sim 1234" is used, where 1234 is a unique random number seed.  There is also a special seed number \verb"-sim 000" that generates data with no error.  That is the simulation model is deterministic, the relative abundance data are directly proportional with 0 observation error, and the age-composition data replicates precisely the true vulnerable proportions-at-age.  The simulation model is conditioned on the historical catch data specified in the data file, and the true parameter values used to simulate the data are those that are specified in the control file.

	
		\subsubsection{Estimation performance with perfect information}
		 
When simulating data with perfect information, there are a couple of things that need to be highlighted here when trying to estimate parameters from data that contain no error.  First, the phases for the precision and variance partitioning parameters ($\vartheta$ and $\rho$) should be set to a negative number.  There are no error in the data, therefore, there is no need to estimate the variance terms for the error distributions.  Second, in the control file, the initial value for the precision ($\vartheta$) should be set to an extremely large number (e.g., 4999.999, assuming the upper bound is 5000).  The reason to set this number large is to minimize the slight bias due to the lognormal bias correction in the stock recruitment relationship (i.e., the $-0.5\tau^2$ term in \ref{T4.12}, or \ref{T4.13}). The control file used to simulate the fake data is provided in Appendix \ref{TableSOGsimCtrl}.




		
		\subsubsection{Bias \& precision with observation \& process errors}
To determine bias and precision of parameter estimates when the model is confronted with both observation error and process error, a series of Monte Carlo trials are performed and $\log_2$ ratios are used to measure the distribution of estimated parameters ($\acute{\theta}$) from the true value($\theta$).  The log2 ratio
\[ \log_2\left(\frac{\acute{\theta}}{\theta}\right) \] is zero when $\acute{\theta}=\theta$, is 1 when $2\acute{\theta}=\theta$, and is -1 when $0.5\acute{\theta}=\theta$.  Box plots are used to examine the distribution of 50 trials where a unique random number seed is used for each trial.  For the purposes of the simulation experiments only, we assume that the proportion of the total variance associated with observation error is known ($\rho = 0.25$) and estimate the total variance.  The total precision is set to 2.50 which is equivalent to a total standard deviation of 0.4 (i.e., 1/2.50=0.4).  The control file used for the Monte Carlo procedures is provided in Appendix \ref{TableSOGmcCtrl}, on page \pageref{TableSOGmcCtrl}.


%% ================================================================ %%
\subsection{Comparison of HCAM with \iscam}\label{secMethodsHCAM}
	
	There are a number of different statistical assumptions and structural differences between the previous assessments using HCAM (Herring Catch Age Model) and \iscam.	  Here we briefly summarize the differences and similarities between the two approaches, and we first attempt to formulate the \iscam\ model to be as similar as possible to the last implementation of HCAM used in \cite{Clear2010}.
	
	The objective function in the HCAM model has four major components to it: 1) the likelihood of the age composition data, 2) the likelihood of the commercial catch data, 3) the likelihood of the spawn data, and 4) the prior densities for estimated model parameters.  In the following subsections are more detailed descriptions of how the \iscam\ model was set up to best approximate the HCAM implementation.
	
	For the gillnet fishery, HCAM implements a time varying selectivity scheme as a function of the average weight-at-age. A similar implementation was also developed in \iscam. Alternative options for changes in selectivity will be investigated further later in this paper.  For the remainder of this paper and especially in the Figures, the definitions for Gear is as follows:  Gear 1 = winter purse seine fishery, Gear 2 = seine-roe fishery, Gear 3 = gillnet fishery.
	
	
\subsubsection{Age-composition data}
There are two alternative likelihoods specified for the age-composition data in the HCAM model \citep[see Table 8 in Appendix B in][]{Clear2010};  a multinomial likelihood (T8.1), and a robust normal approximation to the multinomial(T8.2).  In the \iscam\ model, a multivariate logistic negative log-likelihood for age-composition data is used (see equation \ref{eq8} above), with the intention of weighting these data based on the conditional maximum likelihood estimate of the variance.  In addition, we require a minimum observed proportion of at least 2\% in each age class. In years and ages where the observed proportion is less than 2\% the consecutive ages are grouped into a single age-class which reduces the effective number of age-classes (this is some what analogous to a plus group).

\subsubsection{Commercial catch data}
	In the HCAM assessment, commercial catch was assumed to be know with a high degree of certainty; observation errors were assumed lognormal, and the standard deviation specified in the code is fixed at 0.0707 (variance of 0.005) for all three periods. To implement these assumptions in \iscam\ we fix the assumed standard deviation  for the catches in the last phase to 0.0707.% The analogous setup in \iscam\ is to fix the assumed standard deviation for the catches in the last phase to 0.0707.
	
\subsubsection{Spawn survey data}
 	For the Strait of Georgia spawn survey, the assumed standard deviations in HCAM were specified at 0.35 and 0.3 for the pre and post 1988 periods.  To carry out the same assumptions in \iscam\ the relative weights for the pre and post 1988 survey data were fixed at 1.0 and 1.1666, respectively.  In \iscam\ the total error (or precision=1/variance) is estimated and partitioned into components of observation error (spawn survey residuals) and process error (recruitment deviations).  To implement the same observation error and process errors in the \iscam\ model (standard deviations of 0.35 and 0.8, respectively for observation errors and process errors in HCAM) the total precision was fixed at $\vartheta=1/1.15$, and the proportion assigned to observation error was fixed at $\rho = 0.35/1.15$.

\subsubsection{Specification of prior distributions}
	Starting with the prior density for natural mortality in HCAM, the average natural mortality rate is assumed to be normal with a mean of 0.45, and a standard deviation of 0.2 \citep[see Table 3 in][]{Clear2010}.  The average natural mortality rate in \iscam\ is estimated in the log scale; using a normal prior for the $\ln(M)$ is equivalent to a lognormal prior for $M$.  A lognormal prior is appropriate for this parameter as natural mortality rates must be positive; however, there is no equivalent analytical transformation to the normal distribution that was used in the HCAM assessment.  Here we have specified a  normal prior for $\ln(M)$ with a log mean of $\ln(0.45)=-0.7985$ and a log standard deviation of 0.4 to approximate the variance specified in the normal distribution used in the previous HCAM assessment.
	
	The base HCAM model also allows for a random walk in natural mortality rate implemented as:
\[
M_t =\begin{cases}
	 \psi, \quad t=t'\\
	 M_{t-1}\exp(d_t^M), t>t'
	 \end{cases}
\]
where $d_t^M$ are annual natural mortality deviations that are assumed to be normally distributed with a mean 0 and a standard deviation of 0.10, and $\psi$ is an estimated initial value for natural mortality.  The implementation of time varying natural mortality is similar in \iscam\ in that it is a random walk process, but the components of the objective function include a prior for the initial value of $M$ (as specified in the previous paragraph) and that the first differences between natural mortality deviations are normally distributed. Again, this structure allows natural mortality rates to drift away from central tendency and long-term changes in $M$ could have profound effects on reference point calculations.  For the comparison with the HCAM model, the first differences in annual natural mortality deviations were assumed to have a mean 0 and a standard deviation of 0.10.

Annual recruitment deviations in the HCAM implementation were assumed to be normally distributed on a log scale with a mean of zero and a standard deviation of 0.8.  To set up an equivalent assumption in \iscam, the total variance ($\vartheta$) and ratio of the total variance ($\rho$) that explains observation error in the spawn survey must be specified \emph{a priori}. In the HCAM model the variance terms for the observation errors and process errors are not estimated and assumed to be known; the standard deviation for recruitment variation was set at 0.8 and the standard deviation for observation errors in the spawn survey was fixed at 0.35 and 0.3 for the pre and post 1988 data, respectively.  These variance terms can be estimated within the \iscam\ model, or treated as fixed constants; however, \iscam\ estimates the total error and partitions the variance into observation ($\sigma^2$) and process error ($\tau^2$) components.  To make the same assumptions about the variance terms in \iscam\ as those that were used in HCAM the following values were used $\vartheta=1/1.15=0.8695652$, and $\rho=0.3043478$, and the weights assigned to the post 1988 spawn data were set at 1.1666 and 1.0 for the pre 1988 spawn data.

The prior for steepness in the HCAM model was based on a lognormal distribution with a logmean of 0.67 and a standard deviation of 0.17.  For the Beverton-Holt stock recruitment model, steepness must lie in the interval of  $0.2<h\leq1.0$; a Beta distribution is an appropriate density function for this parameter. In the \iscam\ implementation, a Beta distribution is used and to approximate the distribution used in the HCAM model, the shape and rate parameters specified are 10.0 and 4.925373, respectively.  These values corresponds to a mean of 0.67 and a standard deviation of 0.1178 for the Beta prior.

The last informative prior that is not explicit in the table of priors in the HCAM model is the scaling parameter ($q$) for the spawn survey.  The spawn survey data are broken into two separate time series, pre and post 1988 when the survey switch from a surface estimate to dive surveys for estimating total egg deposition.  In the HCAM implementation, a very informative prior for $q$ in the post 1988 period was used where the mean was fixed at $q=1.0$ and not permitted to vary (i.e., $\sigma_q=0$).  The scaling parameter in the first period was then freely estimated. Again, to emulate these assumptions in the \iscam\ implementation a normal prior for $\ln(q)$ with a mean =0 and a standard deviation of 0.001 was used for the post 1988 data and a uniform prior for the pre 1988 data.

		
%	\subsection{Retrospective analysis}
%	A retrospective analysis was conducted for each of the major and minor SARs.  The retrospective analysis successively removes the last 10-years of data and examines changes in estimates of spawning biomass and estimated reference points.  
%	
%	\subsection{Abundance and recruitment forecasts}
%	The abundance forecast for the upcoming fishing season, also referred to as pre-fishery biomass, is defined as the predicted biomass of age-4 fish and older plus the number of age-3 fish recruiting in year $T+1$.  The abundance estimates are based on the median values from the sampled posterior distribution.  Age-3 recruits are based on poor, average, and good recruitment scenarios; see next paragraph for definitions of poor, average and good.
%	
%	The recruitment forecasts are based on the surviving number of age-3 fish at the start of the fishing season times the average weight-at-age 3 in the last 5 years. The definitions of poor, average, and good recruitment are as follows: \textbf{Poor} is the average recruitment from the 0-33 percentile, \textbf{Average} is the average recruitment from the 33-66 percentile, and \textbf{Good} is the average recruitment from the 66-100 percentile.  Note that all cohorts from 1951 to 2010  were included in the recruitment forecast calculation.	

	
	
